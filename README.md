
<p>
<a href="https://console.tiyaro.ai/explore?q=hfl/&pub=hfl"> <img src="https://tiyaro-public-docs.s3.us-west-2.amazonaws.com/assets/try_on_tiyaro_badge.svg"></a>
</p>

[**ç®€ä½“ä¸­æ–‡**](https://github.com/ymcui/MacBERT) | [**English**](./README_EN.md)

<p align="center">
    <br>
    <img src="./pics/banner.png" width="500"/>
    <br>
</p>
<p align="center">
    <a href="https://github.com/ymcui/MacBERT/blob/master/LICENSE">
        <img alt="GitHub" src="https://img.shields.io/github/license/ymcui/MacBERT.svg?color=blue&style=flat-square">
    </a>
</p>
æœ¬ç›®å½•åŒ…å«**MacBERTé¢„è®­ç»ƒæ¨¡å‹**ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç§çº é”™å‹æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMacï¼‰é¢„è®­ç»ƒä»»åŠ¡ï¼Œç¼“è§£äº†â€œé¢„è®­ç»ƒ-ä¸‹æ¸¸ä»»åŠ¡â€ä¸ä¸€è‡´çš„é—®é¢˜ã€‚MacBERTåœ¨å¤šç§NLPä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚


- **[Revisiting Pre-trained Models for Chinese Natural Language Processing](https://www.aclweb.org/anthology/2020.findings-emnlp.58)**  
- *Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu*
- Published in *Findings of EMNLP 2020*

----

[ä¸­æ–‡MacBERT](https://github.com/ymcui/MacBERT) | [ä¸­æ–‡ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [ä¸­æ–‡XLNet](https://github.com/ymcui/Chinese-XLNet) | [çŸ¥è¯†è’¸é¦å·¥å…·TextBrewer](https://github.com/airaria/TextBrewer) | [æ¨¡å‹è£å‰ªå·¥å…·TextPruner](https://github.com/airaria/TextPruner)

æ›´å¤šHFLå‘å¸ƒçš„èµ„æºï¼šhttps://github.com/ymcui/HFL-Anthology

## News
**2022/3/30 å‘å¸ƒäº†æ–°çš„é¢„è®­ç»ƒæ¨¡å‹PERTï¼šhttps://github.com/ymcui/PERT** 

2021/12/17 å‘å¸ƒäº†æ¨¡å‹è£å‰ªå·¥å…·TextPrunerï¼šhttps://github.com/airaria/TextPruner

2021/10/24 å‘å¸ƒäº†é¦–ä¸ªé¢å‘å°‘æ•°æ°‘æ—è¯­è¨€çš„é¢„è®­ç»ƒæ¨¡å‹CINOï¼šhttps://github.com/ymcui/Chinese-Minority-PLM

2021/7/21  ["è‡ªç„¶è¯­è¨€å¤„ç†ï¼šåŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•"](https://item.jd.com/13344628.html) ä¸€ä¹¦æ­£å¼å‡ºç‰ˆã€‚

2020/11/3 é¢„è®­ç»ƒå¥½çš„ä¸­æ–‡MacBERTå·²å‘å¸ƒï¼Œä½¿ç”¨æ–¹æ³•ä¸BERTä¸€è‡´ã€‚

2020/9/15 è®ºæ–‡["Revisiting Pre-Trained Models for Chinese Natural Language Processing"](https://arxiv.org/abs/2004.13922) è¢«[Findings of EMNLP](https://2020.emnlp.org) å½•ç”¨ä¸ºé•¿æ–‡ã€‚


## ç›®å½•
| ç« èŠ‚ | æè¿° |
|-|-|
| [ç®€ä»‹](#ç®€ä»‹) | ç®€è¦ä»‹ç»MacBERT |
| [ä¸‹è½½](#ä¸‹è½½) | ä¸‹è½½MacBERT |
| [å¿«é€ŸåŠ è½½](#å¿«é€ŸåŠ è½½) | ä»‹ç»å¦‚ä½•ä½¿ç”¨ [ğŸ¤—Transformers](https://github.com/huggingface/transformers) å¿«é€ŸåŠ è½½æ¨¡å‹ |
| [åŸºçº¿æ•ˆæœ](#åŸºçº¿æ•ˆæœ) | åœ¨ä¸­æ–‡NLPä»»åŠ¡ä¸Šçš„æ•ˆæœ |
| [FAQ](#FAQ) | å¸¸è§é—®é¢˜ |
| [å¼•ç”¨](#å¼•ç”¨) | æ–‡ç« å¼•ç”¨ä¿¡æ¯ |


## ç®€ä»‹
**MacBERT** æ˜¯BERTçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå¼•å…¥äº†çº é”™å‹æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLM as correctionï¼ŒMacï¼‰é¢„è®­ç»ƒä»»åŠ¡ï¼Œç¼“è§£äº†â€œé¢„è®­ç»ƒ-ä¸‹æ¸¸ä»»åŠ¡â€ä¸ä¸€è‡´çš„é—®é¢˜ã€‚

æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ä¸­ï¼Œå¼•å…¥äº†[MASK]æ ‡è®°è¿›è¡Œæ©ç ï¼Œä½†[MASK]æ ‡è®°å¹¶ä¸ä¼šå‡ºç°åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ã€‚åœ¨MacBERTä¸­ï¼Œ**æˆ‘ä»¬ä½¿ç”¨ç›¸ä¼¼è¯æ¥å–ä»£[MASK]æ ‡è®°**ã€‚ç›¸ä¼¼è¯é€šè¿‡[Synonyms toolkit (Wang and Hu, 2017)](https://github.com/chatopera/Synonyms)å·¥å…·è·å–ï¼Œç®—æ³•åŸºäºword2vec (Mikolov et al., 2013)ç›¸ä¼¼åº¦è®¡ç®—ã€‚åŒæ—¶æˆ‘ä»¬ä¹Ÿå¼•å…¥äº†Whole Word Maskingï¼ˆwwmï¼‰å’ŒN-gram maskingæŠ€æœ¯ã€‚å½“è¦å¯¹N-gramè¿›è¡Œæ©ç æ—¶ï¼Œæˆ‘ä»¬ä¼šå¯¹N-gramé‡Œçš„æ¯ä¸ªè¯åˆ†åˆ«æŸ¥æ‰¾ç›¸ä¼¼è¯ã€‚å½“æ²¡æœ‰ç›¸ä¼¼è¯å¯æ›¿æ¢æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨éšæœºè¯è¿›è¡Œæ›¿æ¢ã€‚

ä»¥ä¸‹æ˜¯è®­ç»ƒæ ·æœ¬ç¤ºä¾‹ã€‚

|  | ä¾‹å­       |
| -------------- | ----------------- |
| **åŸå§‹å¥å­** | we use a language model to predict the probability of the next word. |
|  **MLM** | we use a language [M] to [M] ##di ##ct the pro [M] ##bility of the next word . |
| **Whole word masking**   | we use a language [M] to [M] [M] [M] the [M] [M] [M] of the next word . |
| **N-gram masking** | we use a [M] [M] to [M] [M] [M] the [M] [M] [M] [M] [M] next word . |
| **MLM as correction** | we use a text system to ca ##lc ##ulate the po ##si ##bility of the next word . |

**MacBERTçš„ä¸»è¦æ¡†æ¶ä¸BERTå®Œå…¨ä¸€è‡´ï¼Œå¯åœ¨ä¸ä¿®æ”¹ç°æœ‰ä»£ç çš„åŸºç¡€ä¸Šè¿›è¡Œæ— ç¼è¿‡æ¸¡ã€‚**

æ›´å¤šç»†èŠ‚è¯·å‚è€ƒæˆ‘ä»¬çš„è®ºæ–‡ï¼š**[Revisiting Pre-trained Models for Chinese Natural Language Processing](https://www.aclweb.org/anthology/2020.findings-emnlp.58)**  


## ä¸‹è½½
ä¸»è¦æä¾›TensorFlow 1.xç‰ˆæœ¬çš„æ¨¡å‹ä¸‹è½½ã€‚

* **`MacBERT-large, Chinese`**: 24-layer, 1024-hidden, 16-heads, 324M parameters   
* **`MacBERT-base, Chinese`**ï¼š12-layer, 768-hidden, 12-heads, 102M parameters   

| æ¨¡å‹                                |                         Google Drive                         |                        ç™¾åº¦ç›˜                        | å¤§å° |
| :----------------------------------- | :----------------------------------------------------------: | :----------------------------------------------------------: | :--: |
| **`MacBERT-large, Chinese`**    | [TensorFlow](https://drive.google.com/file/d/1lWYxnk1EqTA2Q20_IShxBrCPc5VSDCkT/view?usp=sharing) | [TensorFlowï¼ˆpw:zejfï¼‰](https://pan.baidu.com/s/1nJEjhUAnWGO_1RPki21mxA?pwd=zejf) | 1.2G |
| **`MacBERT-base, Chinese`**     | [TensorFlow](https://drive.google.com/file/d/1aV69OhYzIwj_hn-kO1RiBa-m8QAusQ5b/view?usp=sharing) | [TensorFlowï¼ˆpw:61gaï¼‰](https://pan.baidu.com/s/1EAs2fmraqtvfia5Q5rXnuQ?pwd=61ga) | 383M |

### PyTorch/TensorFlow2 ç‰ˆæœ¬

å¦‚æœéœ€è¦PyTorchæˆ–è€…TensorFlow2ç‰ˆæœ¬çš„æ¨¡å‹ï¼š

1. ä½¿ç”¨  [ğŸ¤—Transformers](https://github.com/huggingface/transformers) è‡ªè¡Œè½¬æ¢
2. æˆ–è€…ä» https://huggingface.co/hfl ä¸‹è½½

ä¸‹è½½æ­¥éª¤ï¼ˆä¹Ÿå¯ä»¥ç›´æ¥ç”¨gitå°†æ•´ä¸ªç›®å½•å…‹éš†ä¸‹æ¥ï¼‰ï¼š

1. è¿›å…¥https://huggingface.co/hfl ä¹‹åé€‰æ‹©æŸä¸ªMacBERTæ¨¡å‹ï¼Œä¾‹å¦‚MacBERT-baseï¼šhttps://huggingface.co/hfl/chinese-macbert-base
2. é€‰æ‹©"files and versions"é€‰é¡¹å¡
3. ç‚¹å‡»éœ€è¦ä¸‹è½½çš„bin/jsonç­‰æ–‡ä»¶


## å¿«é€ŸåŠ è½½
é€šè¿‡  [ğŸ¤—Transformers](https://github.com/huggingface/transformers)  å¯ä»¥å¿«é€ŸåŠ è½½MacBERTæ¨¡å‹ã€‚

```
tokenizer = BertTokenizer.from_pretrained("MODEL_NAME")
model = BertModel.from_pretrained("MODEL_NAME")
```
**æ³¨æ„ï¼šè¯·ä½¿ç”¨BertTokenizerå’ŒBertModelæ¥åŠ è½½MacBERTæ¨¡å‹ï¼**

å¯¹åº”çš„`MODEL_NAME` å¦‚ä¸‹æ‰€ç¤ºï¼š

| åŸæ¨¡å‹        | æ¨¡å‹è°ƒç”¨å                |
| ------------- | ------------------------- |
| MacBERT-large | hfl/chinese-macbert-large |
| MacBERT-base  | hfl/chinese-macbert-base  |

## åŸºçº¿æ•ˆæœ
è¿™é‡Œå±•ç¤ºMacBERTåœ¨6ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ•ˆæœï¼ˆæ›´å¤šç»“æœè¯·å‚è€ƒè®ºæ–‡ï¼‰ï¼š

- [**CMRC 2018 (Cui et al., 2019)**ï¼šæŠ½å–å¼é˜…è¯»ç†è§£ï¼ˆç®€ä½“ä¸­æ–‡ï¼‰](https://github.com/ymcui/cmrc2018)
- [**DRCD (Shao et al., 2018)**ï¼šæŠ½å–å¼é˜…è¯»ç†è§£ï¼ˆç¹ä½“ä¸­æ–‡ï¼‰](https://github.com/DRCSolutionService/DRCD)
- [**XNLI (Conneau et al., 2018)**ï¼šè‡ªç„¶è¯­è¨€æ¨æ–­](https://github.com/google-research/bert/blob/master/multilingual.md)
- [**ChnSentiCorp**ï¼šæƒ…æ„Ÿåˆ†ç±»](https://github.com/pengming617/bert_classification)
- [**LCQMC (Liu et al., 2018)**ï¼šå¥å¯¹åŒ¹é…](http://icrc.hitsz.edu.cn/info/1037/1146.htm)
- [**BQ Corpus (Chen et al., 2018)**ï¼šå¥å¯¹åŒ¹é…](http://icrc.hitsz.edu.cn/Article/show/175.html)

ä¸ºäº†ä¿è¯ç»“æœçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬åŒæ—¶ç»™å‡ºç‹¬ç«‹è¿è¡Œ10æ¬¡çš„å¹³å‡å€¼ï¼ˆæ‹¬å·å†…ï¼‰å’Œæœ€å¤§å€¼ã€‚

### CMRC 2018
[**CMRC 2018æ•°æ®é›†**](https://github.com/ymcui/cmrc2018)æ˜¯å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤å‘å¸ƒçš„ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£æ•°æ®ã€‚
æ ¹æ®ç»™å®šé—®é¢˜ï¼Œç³»ç»Ÿéœ€è¦ä»ç¯‡ç« ä¸­æŠ½å–å‡ºç‰‡æ®µä½œä¸ºç­”æ¡ˆï¼Œå½¢å¼ä¸SQuADç›¸åŒã€‚
è¯„æµ‹æŒ‡æ ‡ä¸ºï¼šEM / F1

| Model                     |        Development        |           Test            |         Challenge         | #Params |
| :------------------------ | :-----------------------: | :-----------------------: | :-----------------------: | :-----: |
| BERT-base                 | 65.5 (64.4) / 84.5 (84.0) | 70.0 (68.7) / 87.0 (86.3) | 18.6 (17.0) / 43.3 (41.3) |  102M   |
| BERT-wwm                  | 66.3 (65.0) / 85.6 (84.7) | 70.5 (69.1) / 87.4 (86.7) | 21.0 (19.3) / 47.0 (43.9) |  102M   |
| BERT-wwm-ext              | 67.1 (65.6) / 85.7 (85.0) | 71.4 (70.0) / 87.7 (87.0) | 24.0 (20.0) / 47.3 (44.6) |  102M   |
| RoBERTa-wwm-ext           | 67.4 (66.5) / 87.2 (86.5) | 72.6 (71.4) / 89.4 (88.8) | 26.2 (24.6) / 51.0 (49.1) |  102M   |
| ELECTRA-base          | 68.4 (68.0) / 84.8 (84.6) | 73.1 (72.7) / 87.1 (86.9) | 22.6 (21.7) / 45.0 (43.8) |  102M   |
| **MacBERT-base** | 68.5 (67.3) / 87.9 (87.1) |73.2 (72.4) / 89.5 (89.2)|30.2 (26.4) / 54.0 (52.2)|102M|
| ELECTRA-large         |        69.1 (68.2) / 85.2 (84.5)        |        73.9 (72.8) / 87.1 (86.6)        |        23.0 (21.6) / 44.2 (43.2)        |  324M   |
| RoBERTa-wwm-ext-large | 68.5 (67.6) / 88.4 (87.9) | 74.2 (72.4) / 90.6 (90.0) | 31.5 (30.1) / 60.1 (57.5) |324M|
| **MacBERT-large** | 70.7 (68.6) / 88.9 (88.2) | 74.8 (73.2) / 90.7 (90.1) | 31.9 (29.6) / 60.2 (57.6) | 324M |


### DRCD
[**DRCDæ•°æ®é›†**](https://github.com/DRCKnowledgeTeam/DRCD)ç”±ä¸­å›½å°æ¹¾å°è¾¾ç ”ç©¶é™¢å‘å¸ƒï¼Œå…¶å½¢å¼ä¸SQuADç›¸åŒï¼Œæ˜¯åŸºäºç¹ä½“ä¸­æ–‡çš„æŠ½å–å¼é˜…è¯»ç†è§£æ•°æ®é›†ã€‚
**ç”±äºERNIEä¸­å»é™¤äº†ç¹ä½“ä¸­æ–‡å­—ç¬¦ï¼Œæ•…ä¸å»ºè®®åœ¨ç¹ä½“ä¸­æ–‡æ•°æ®ä¸Šä½¿ç”¨ERNIEï¼ˆæˆ–è½¬æ¢æˆç®€ä½“ä¸­æ–‡åå†å¤„ç†ï¼‰ã€‚**
è¯„æµ‹æŒ‡æ ‡ä¸ºï¼šEM / F1

| Model                     |        Development        |           Test            | #Params |
| :------------------------ | :-----------------------: | :-----------------------: | :-----: |
| BERT-base                 | 83.1 (82.7) / 89.9 (89.6) | 82.2 (81.6) / 89.2 (88.8) |  102M   |
| BERT-wwm                  | 84.3 (83.4) / 90.5 (90.2) | 82.8 (81.8) / 89.7 (89.0) |  102M   |
| BERT-wwm-ext              | 85.0 (84.5) / 91.2 (90.9) | 83.6 (83.0) / 90.4 (89.9) |  102M   |
| RoBERTa-wwm-ext           | 86.6 (85.9) / 92.5 (92.2) | 85.6 (85.2) / 92.0 (91.7) |  102M   |
| ELECTRA-base          | 87.5 (87.0) / 92.5 (92.3) | 86.9 (86.6) / 91.8 (91.7) |  102M   |
| **MacBERT-base** | 89.4 (89.2) / 94.3 (94.1) | 89.5 (88.7) / 93.8 (93.5) | 102M |
| ELECTRA-large         |        88.8 (88.7) / 93.3 (93.2)        |        88.8 (88.2) / 93.6 (93.2)        |  324M   |
| RoBERTa-wwm-ext-large | 89.6 (89.1) / 94.8 (94.4) | 89.6 (88.9) / 94.5 (94.1) |324M|
| **MacBERT-large** | 91.2 (90.8) / 95.6 (95.3) | 91.7 (90.9) / 95.6 (95.3) |324M|


### XNLI
åœ¨è‡ªç„¶è¯­è¨€æ¨æ–­ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†[**XNLI**æ•°æ®](https://github.com/google-research/bert/blob/master/multilingual.md)ï¼Œéœ€è¦å°†æ–‡æœ¬åˆ†æˆä¸‰ä¸ªç±»åˆ«ï¼š`entailment`ï¼Œ`neutral`ï¼Œ`contradictory`ã€‚
è¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy

| Model                     | Development |    Test     | #Params |
| :------------------------ | :---------: | :---------: | :-----: |
| BERT-base                 | 77.8 (77.4) | 77.8 (77.5) |  102M   |
| BERT-wwm                  | 79.0 (78.4) | 78.2 (78.0) |  102M   |
| BERT-wwm-ext              | 79.4 (78.6) | 78.7 (78.3) |  102M   |
| RoBERTa-wwm-ext           | 80.0 (79.2) | 78.8 (78.3) |  102M   |
| ELECTRA-base          | 77.9 (77.0) | 78.4 (77.8) |  102M   |
| **MacBERT-base** | 80.3 (79.7) | 79.3 (78.8) | 102M |
| ELECTRA-large         |    81.5 (80.8)    |    81.0 (80.9)    |  324M   |
| RoBERTa-wwm-ext-large | 82.1 (81.3) | 81.2 (80.6) |324M|
| **MacBERT-large** | 82.4 (81.8) | 81.3 (80.6) |324M|


### ChnSentiCorp
åœ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­ï¼ŒäºŒåˆ†ç±»çš„æƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†ChnSentiCorpã€‚
è¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy

| Model                     | Development |    Test     | #Params |
| :------------------------ | :---------: | :---------: | :-----: |
| BERT-base                 | 94.7 (94.3) | 95.0 (94.7) |  102M   |
| BERT-wwm                  | 95.1 (94.5) | 95.4 (95.0) |  102M   |
| BERT-wwm-ext              | 95.4 (94.6) | 95.3 (94.7) |  102M   |
| RoBERTa-wwm-ext           | 95.0 (94.6) | 95.6 (94.8) |  102M   |
| ELECTRA-base          | 93.8 (93.0) | 94.5 (93.5) |  102M   |
| **MacBERT-base** | 95.2 (94.8) | 95.6 (94.9) | 102M |
| ELECTRA-large         |    95.2 (94.6)    |    95.3 (94.8)    |  324M   |
| RoBERTa-wwm-ext-large | 95.8 (94.9) | 95.8 (94.9) |324M|
| **MacBERT-large** | 95.7 (95.0) | 95.9 (95.1) | 324M |


### LCQMC
[LCQMC](http://icrc.hitsz.edu.cn/info/1037/1146.htm)ç”±å“ˆå·¥å¤§æ·±åœ³ç ”ç©¶ç”Ÿé™¢æ™ºèƒ½è®¡ç®—ç ”ç©¶ä¸­å¿ƒå‘å¸ƒã€‚ 
è¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy

| Model                     | Development |    Test     | #Params |
| :------------------------ | :---------: | :---------: | :-----: |
| BERT                      | 89.4 (88.4) | 86.9 (86.4) |  102M   |
| BERT-wwm                  | 89.4 (89.2) | 87.0 (86.8) |  102M   |
| BERT-wwm-ext              | 89.6 (89.2) | 87.1 (86.6) |  102M   |
| RoBERTa-wwm-ext           | 89.0 (88.7) | 86.4 (86.1) |  102M   |
| ELECTRA-base          | 90.2 (89.8) | 87.6 (87.3) |  102M   |
| **MacBERT-base** | 89.5 (89.3) | 87.0 (86.5) | 102M |
| ELECTRA-large         |    90.7 (90.4)    |    87.3 (87.2)    |  324M   |
| RoBERTa-wwm-ext-large | 90.4 (90.0) | 87.0 (86.8) |324M|
| **MacBERT-large** | 90.6 (90.3) | 87.6 (87.1) | 324M |

### BQ Corpus 
[BQ Corpus](http://icrc.hitsz.edu.cn/Article/show/175.html)ç”±å“ˆå·¥å¤§æ·±åœ³ç ”ç©¶ç”Ÿé™¢æ™ºèƒ½è®¡ç®—ç ”ç©¶ä¸­å¿ƒå‘å¸ƒï¼Œæ˜¯é¢å‘é“¶è¡Œé¢†åŸŸçš„æ•°æ®é›†ã€‚
è¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy

| Model                     | Development |    Test     | #Params |
| :------------------------ | :---------: | :---------: | :-----: |
| BERT                      | 86.0 (85.5) | 84.8 (84.6) |  102M   |
| BERT-wwm                  | 86.1 (85.6) | 85.2 (84.9) |  102M   |
| BERT-wwm-ext              | 86.4 (85.5) | 85.3 (84.8) |  102M   |
| RoBERTa-wwm-ext           | 86.0 (85.4) | 85.0 (84.6) |  102M   |
| ELECTRA-base          | 84.8 (84.7) | 84.5 (84.0) |  102M   |
| **MacBERT-base** | 86.0 (85.5) | 85.2 (84.9) | 102M |
| ELECTRA-large         |    86.7 (86.2)    |    85.1 (84.8)    |  324M   |
| RoBERTa-wwm-ext-large | 86.3 (85.7) | 85.8 (84.9) |324M|
| **MacBERT-large** | 86.2 (85.7) | 85.6 (85.0) | 324M |

## FAQ
**Q1: æœ‰è‹±æ–‡ç‰ˆçš„MacBERTå—ï¼Ÿ**

A1: ç›®å‰æ²¡æœ‰ã€‚

**Q2: å¦‚ä½•ä½¿ç”¨MacBERTï¼Ÿ**

A2: å’Œä½¿ç”¨BERTä¸€æ ·ï¼Œåªéœ€è¦ç®€å•æ›¿æ¢æ¨¡å‹æ–‡ä»¶å’Œconfigå°±èƒ½ä½¿ç”¨äº†ã€‚å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆå³åˆå§‹åŒ–transformerséƒ¨åˆ†ï¼‰æ¥è¿›ä¸€æ­¥è®­ç»ƒå…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ã€‚

**Q3: èƒ½æä¾›MacBERTçš„è®­ç»ƒä»£ç å—ï¼Ÿ**

A3: æš‚æ— å¼€æºè®¡åˆ’ã€‚

**Q4: èƒ½å¼€æºé¢„è®­ç»ƒçš„è¯­æ–™å—ï¼Ÿ**

A4: æˆ‘ä»¬æ— æ³•å¼€æºè®­ç»ƒè¯­æ–™ï¼Œå› ä¸ºæ²¡æœ‰ç›¸åº”é‡å‘å¸ƒçš„æƒåˆ©ã€‚GitHubä¸Šæœ‰ä¸€äº›å¼€æºä¸­æ–‡è¯­æ–™èµ„æºï¼Œå¯ä»¥å¤šåŠ å…³æ³¨åˆ©ç”¨ã€‚

**Q5: æœ‰è®¡åˆ’åœ¨æ›´å¤§çš„è¯­æ–™ä¸Šè®­ç»ƒMacBERTå¹¶å¼€æºå—ï¼Ÿ**

A5: æˆ‘ä»¬æš‚æ—¶æ²¡æœ‰è®¡åˆ’ã€‚

## å¼•ç”¨
å¦‚æœæœ¬é¡¹ç›®ä¸­çš„èµ„æºå¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ã€‚

```
@inproceedings{cui-etal-2020-revisiting,
    title = "Revisiting Pre-Trained Models for {C}hinese Natural Language Processing",
    author = "Cui, Yiming  and
      Che, Wanxiang  and
      Liu, Ting  and
      Qin, Bing  and
      Wang, Shijin  and
      Hu, Guoping",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.58",
    pages = "657--668",
}
```

æˆ–è€…ï¼š
```
@journal{cui-etal-2021-pretrain,
  title={Pre-Training with Whole Word Masking for Chinese BERT},
  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},
  journal={IEEE Transactions on Audio, Speech and Language Processing},
  year={2021},
  url={https://ieeexplore.ieee.org/document/9599397},
  doi={10.1109/TASLP.2021.3124365},
 }
```

## è‡´è°¢
æ„Ÿè°¢Google [TPU Research Cloud (TFRC)](https://www.tensorflow.org/tfrc)æä¾›è®¡ç®—èµ„æºæ”¯æŒã€‚



## é—®é¢˜åé¦ˆ
å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚

- åœ¨æäº¤é—®é¢˜ä¹‹å‰ï¼Œè¯·å…ˆæŸ¥çœ‹FAQèƒ½å¦è§£å†³é—®é¢˜ï¼ŒåŒæ—¶å»ºè®®æŸ¥é˜…ä»¥å¾€çš„issueæ˜¯å¦èƒ½è§£å†³ä½ çš„é—®é¢˜ã€‚
- é‡å¤ä»¥åŠä¸æœ¬é¡¹ç›®æ— å…³çš„issueä¼šè¢«[stable-bot](stale Â· GitHub Marketplace)å¤„ç†ï¼Œæ•¬è¯·è°…è§£ã€‚
- æˆ‘ä»¬ä¼šå°½å¯èƒ½çš„è§£ç­”ä½ çš„é—®é¢˜ï¼Œä½†æ— æ³•ä¿è¯ä½ çš„é—®é¢˜ä¸€å®šä¼šè¢«è§£ç­”ã€‚
- ç¤¼è²Œåœ°æå‡ºé—®é¢˜ï¼Œæ„å»ºå’Œè°çš„è®¨è®ºç¤¾åŒºã€‚
